# -*- coding: utf-8 -*-
"""Generalizacion_a_Escenarios_Variables_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJqfxwQT9HJZFGxR1QfZt2nZrRafiJ7n
"""

!pip install -e "git+https://github.com/openskynetwork/opensky-api.git#egg=opensky_api&subdirectory=python"
!pip uninstall opensky-api
!pip install dtale
!pip install contextily geopandas
!pip install opensky-api
!pip show opensky-api
import sys
sys.path.append("./src/opensky-api/python")  # Adjust path if needed

from opensky_api import OpenSkyApi  # Now this should work

import sqlite3
from typing import List, Optional, Tuple
from opensky_api import OpenSkyApi, StateVector

# Configuración de la base de datos
DATABASE_NAME = 'Data2.db'
TABLE_NAME = 'flight_data2'

def create_database_connection(db_name: str) -> sqlite3.Connection:
    """Crea y retorna una conexión a la base de datos SQLite."""
    try:
        conn = sqlite3.connect(db_name)
        conn.execute('PRAGMA journal_mode = WAL')
        return conn
    except sqlite3.Error as e:
        print(f"Error creando conexión a la base de datos: {e}")
        raise

def create_table(conn: sqlite3.Connection) -> None:
    """Crea la tabla si no existe con comprobación de tipos de datos."""
    create_table_query = f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
        time INTEGER NOT NULL,
        icao24 TEXT NOT NULL,
        latitude REAL,
        longitude REAL,
        velocity REAL,
        heading REAL,
        vertrate REAL,
        onground BOOLEAN,
        spi BOOLEAN,
        baro_altitude REAL,
        geo_altitude REAL,
        lastposupdate INTEGER,
        lastcontact INTEGER,
        PRIMARY KEY (lastposupdate, icao24)
    )
    """
    try:
        conn.execute(create_table_query)
        conn.commit()
    except sqlite3.Error as e:
        print(f"Error creando tabla: {e}")
        raise

def validate_state(state: StateVector) -> Optional[Tuple[Optional[int], str, Optional[float], Optional[float], Optional[float], Optional[float], Optional[float], bool, bool, Optional[float], Optional[float], Optional[int], Optional[int]]]:
    """
    Valida y convierte los datos del estado a tipos apropiados para SQLite.

    Retorna una tupla con los valores:
    (time, icao24, latitude, longitude, velocity, heading, vertrate, onground, spi, baro_altitude, geo_altitude, lastposupdate, lastcontact)

    Se utiliza un valor placeholder (None) para 'time' que se asignará en save_to_database con default_timestamp.
    """
    try:
        # Se obtiene el valor de time_position, que se usará para lastposupdate; si no está disponible se asignará más adelante.
        lastposupdate = state.time_position if state.time_position else None
        return (
            None,                # Placeholder para 'time'
            state.icao24,        # Identificador único de la aeronave
            state.latitude,
            state.longitude,
            state.velocity,
            state.true_track,
            state.vertical_rate,
            bool(state.on_ground),
            bool(state.spi),
            state.baro_altitude,
            state.geo_altitude,
            lastposupdate,
            state.last_contact
        )
    except AttributeError as e:
        print(f"Datos inválidos en el estado: {e}")
        return None

def save_to_database(conn: sqlite3.Connection, states: List[StateVector], default_timestamp: int) -> None:
    """
    Guarda los estados en la base de datos en lotes.
    Si un estado no tiene timestamp (time_position), se utiliza default_timestamp para la columna 'time'.
    """
    data = []
    for state in states:
        if validated := validate_state(state):
            # Si el valor de 'time' es None, se utiliza default_timestamp
            ts = validated[0] if validated[0] is not None else default_timestamp
            data.append((ts,) + validated[1:])

    insert_query = f"""
    INSERT OR IGNORE INTO {TABLE_NAME} (
        time, icao24, latitude, longitude, velocity, heading, vertrate, onground, spi, baro_altitude, geo_altitude, lastposupdate, lastcontact
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    try:
        with conn:
            conn.executemany(insert_query, data)
        print(f"{len(data)} registros insertados exitosamente")
    except sqlite3.Error as e:
        print(f"Error insertando datos: {e}")
        raise

def main():
    """Función principal que extrae datos de la API y los guarda en la base de datos."""
    try:
        # Si cuentas con credenciales, reemplaza 'YOUR_USERNAME' y 'YOUR_PASSWORD'
        api = OpenSkyApi()
    except Exception as e:
        print(f"Error inicializando la API: {e}")
        return

    try:
        states_response = api.get_states()  # time_secs=0 es el valor por defecto para obtener los datos más recientes
        if not states_response or not states_response.states:
            print("No se recibieron datos válidos de la API.")
            return
    except Exception as e:
        print(f"Error obteniendo datos de la API: {e}")
        return

    try:
        conn = create_database_connection(DATABASE_NAME)
        create_table(conn)
        save_to_database(conn, states_response.states, states_response.time)
        conn.close()
    except Exception as e:
        print(f"Error en el proceso de base de datos: {e}")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import sqlite3

# 1. Carga de datos desde archivo .db con verificación de la existencia de la tabla
def load_data(filepath, table_name='flight_data2'):
    with sqlite3.connect(filepath) as conn:
        # Listar tablas disponibles en la base de datos
        available_tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn)['name'].tolist()
        print("Tablas disponibles en la base de datos:", available_tables)
        if table_name not in available_tables:
            raise ValueError(f"La tabla '{table_name}' no existe en la base de datos. Verifica el nombre correcto de la tabla.")
        query = f"SELECT * FROM {table_name}"
        df = pd.read_sql_query(query, conn)
    print("Datos cargados correctamente desde .db")
    return df

# 1.1 Limpieza y normalización de columnas
def clean_data(df):
    # Convertir todos los nombres de columna a minúsculas y eliminar espacios en blanco
    df.columns = df.columns.str.lower().str.strip()

    # Renombrar columnas para tener un formato consistente
    # Si se encuentra 'geoaltitude' sin guión bajo, se renombra a 'geo_altitude'
    if 'geoaltitude' in df.columns and 'geo_altitude' not in df.columns:
        df.rename(columns={'geoaltitude': 'geo_altitude'}, inplace=True)
    if 'baroaltitude' in df.columns and 'baro_altitude' not in df.columns:
        df.rename(columns={'baroaltitude': 'baro_altitude'}, inplace=True)

    # Definir las columnas numéricas a convertir
    columnas_numericas = ['time', 'latitude', 'longitude', 'geo_altitude', 'baro_altitude', 'vertrate', 'velocity', 'heading']
    for col in columnas_numericas:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Verificar que se encuentre una columna de altitud válida
    if 'geo_altitude' not in df.columns or df['geo_altitude'].isna().all():
        if 'baro_altitude' in df.columns and not df['baro_altitude'].isna().all():
            df['geo_altitude'] = df['baro_altitude']
            print("Se creó 'geo_altitude' a partir de 'baro_altitude'.")
        elif 'alt' in df.columns and not df['alt'].isna().all():
            df['geo_altitude'] = df['alt']
            print("Se creó 'geo_altitude' a partir de 'alt'.")
        else:
            raise ValueError("La columna requerida 'geo_altitude' no está presente en el DataFrame.")
    return df

import sqlite3
import pandas as pd

# Cargar los datos desde la base de datos SQLite
conn = sqlite3.connect('Data2.db')

# Cambia 'flight_data2' por el nombre correcto de la tabla
datos = pd.read_sql_query("SELECT * FROM flight_data2", conn)

conn.close()

# Mostrar el DataFrame
print(datos.head())  # Muestra las primeras filas del DataFrame

# 2. Transformación a coordenadas ECEF
def geodetic_to_ecef(lat, lon, alt):
    # Parámetros del elipsoide WGS84
    a = 6378137.0         # semieje mayor [m]
    b = 6356752.3         # semieje menor [m]
    e_squared = 1 - (b**2 / a**2)

    lat_rad = np.radians(lat)
    lon_rad = np.radians(lon)
    N = a / np.sqrt(1 - e_squared * np.sin(lat_rad)**2)

    x = (N + alt) * np.cos(lat_rad) * np.cos(lon_rad)
    y = (N + alt) * np.cos(lat_rad) * np.sin(lon_rad)
    z = (N * (1 - e_squared) + alt) * np.sin(lat_rad)
    return x, y, z

# 3. Interpolación de altitud
def interpolate_missing_altitude(df):
    df_interpolated = df.copy()
    if 'geo_altitude' not in df_interpolated.columns:
        print("No se puede interpolar altitud: columna 'geo_altitude' ausente.")
        return df_interpolated

    missing_alt_mask = df_interpolated['geo_altitude'].isna() & df_interpolated['vertrate'].notna()
    for icao, group in df_interpolated.groupby('icao24'):
        if len(group) < 2:
            continue
        sorted_group = group.sort_values('time')
        for idx in sorted_group.index:
            if missing_alt_mask.loc[idx]:
                prev_records = sorted_group.loc[sorted_group['time'] < sorted_group.loc[idx, 'time']]
                if not prev_records.empty and prev_records['geo_altitude'].notna().any():
                    prev_record = prev_records.iloc[-1]
                    delta_t = sorted_group.loc[idx, 'time'] - prev_record['time']
                    vert_rate = sorted_group.loc[idx, 'vertrate']
                    df_interpolated.loc[idx, 'geo_altitude'] = prev_record['geo_altitude'] + vert_rate * delta_t
                elif 'baro_altitude' in sorted_group.columns and pd.notna(sorted_group.loc[idx, 'baro_altitude']):
                    df_interpolated.loc[idx, 'geo_altitude'] = sorted_group.loc[idx, 'baro_altitude']
    filled_count = df_interpolated['geo_altitude'].notna().sum() - df['geo_altitude'].notna().sum()
    print(f"Se interpolaron {filled_count} valores de altitud faltantes")
    return df_interpolated

# 4. Filtrado de outliers
def filter_outliers(df):
    df_filtered = df.copy()
    velocity_mask = (df_filtered['velocity'] <= 300) & (df_filtered['velocity'] >= 0)
    heading_mask = (df_filtered['heading'] >= 0) & (df_filtered['heading'] < 360)
    original_count = len(df_filtered)
    df_filtered = df_filtered[velocity_mask & heading_mask]
    filtered_count = original_count - len(df_filtered)
    print(f"Se filtraron {filtered_count} registros outliers")
    return df_filtered

def filter_ground_aircraft(df):
    original_count = len(df)
    if 'geo_altitude' not in df.columns:
        print("No se puede filtrar aeronaves en tierra: columna 'geo_altitude' ausente.")
        return df
    df_filtered = df[df['geo_altitude'] >= 100]
    discarded = original_count - len(df_filtered)
    print(f"Se descartaron {discarded} registros con altitud < 100 m")
    return df_filtered

# 5. Dinámica
def calculate_dynamics(df):
    df_dynamic = df.copy()
    for icao, group in df.groupby('icao24'):
        if len(group) < 2:
            continue
        sorted_group = group.sort_values('time')
        sorted_indices = sorted_group.index
        df_dynamic.loc[sorted_indices, 'angular_accel'] = np.nan
        df_dynamic.loc[sorted_indices, 'drag_force'] = np.nan
        for i in range(len(sorted_indices) - 1):
            curr_idx = sorted_indices[i]
            next_idx = sorted_indices[i + 1]
            delta_t = df.loc[next_idx, 'time'] - df.loc[curr_idx, 'time']
            if delta_t > 0:
                curr_heading = df.loc[curr_idx, 'heading']
                next_heading = df.loc[next_idx, 'heading']
                delta_heading = ((next_heading - curr_heading + 180) % 360) - 180
                df_dynamic.loc[curr_idx, 'angular_accel'] = delta_heading / delta_t
                if i == len(sorted_indices) - 2:
                    df_dynamic.loc[next_idx, 'angular_accel'] = delta_heading / delta_t
            velocity = df.loc[curr_idx, 'velocity']
            rho = 1.225  # densidad del aire [kg/m³]
            Cd = 0.027   # coeficiente de arrastre
            A = 120      # área de referencia [m²]
            df_dynamic.loc[curr_idx, 'drag_force'] = 0.5 * rho * Cd * A * velocity ** 2
            if i == len(sorted_indices) - 2:
                velocity_last = df.loc[next_idx, 'velocity']
                df_dynamic.loc[next_idx, 'drag_force'] = 0.5 * rho * Cd * A * velocity_last ** 2
    return df_dynamic

# 6. Transformación a ECEF
def transform_all_coordinates(df):
    df_transformed = df.copy()
    df_transformed['x_ecef'] = np.nan
    df_transformed['y_ecef'] = np.nan
    df_transformed['z_ecef'] = np.nan
    valid_rows = df_transformed['latitude'].notna() & df_transformed['longitude'].notna() & df_transformed['geo_altitude'].notna()
    for idx in df_transformed[valid_rows].index:
        lat = df_transformed.loc[idx, 'latitude']
        lon = df_transformed.loc[idx, 'longitude']
        alt = df_transformed.loc[idx, 'geo_altitude']
        x, y, z = geodetic_to_ecef(lat, lon, alt)
        df_transformed.loc[idx, 'x_ecef'] = x
        df_transformed.loc[idx, 'y_ecef'] = y
        df_transformed.loc[idx, 'z_ecef'] = z
    transformed_count = df_transformed['x_ecef'].notna().sum()
    print(f"Se transformaron {transformed_count} coordenadas a ECEF")
    return df_transformed

# 7. Métricas de validación
def calculate_metrics(df):
    metrics = {}
    time_diffs = []
    for icao, group in df.groupby('icao24'):
        if len(group) < 2:
            continue
        sorted_group = group.sort_values('time')
        time_diff = sorted_group['time'].diff().dropna()
        time_diffs.extend(time_diff.tolist())
    if time_diffs:
        metrics['lastcontact_p95'] = np.percentile(time_diffs, 95)
        print(f"Percentil 95 de tiempo entre contactos: {metrics['lastcontact_p95']:.2f}s")
    if 'geo_altitude' in df.columns:
        alt_above_100m = (df['geo_altitude'] > 100).mean() * 100
        metrics['alt_above_100m_percent'] = alt_above_100m
        print(f"Porcentaje de altitudes > 100 m: {alt_above_100m:.2f}%")
    if 'vertrate' in df.columns:
        vert_rates = df['vertrate'].dropna().abs()
        if not vert_rates.empty:
            metrics['vertrate_mean'] = vert_rates.mean()
            print(f"Media absoluta de la tasa vertical: {metrics['vertrate_mean']:.2f} m/s")
    return metrics

# 8. Visualización
def visualize_trajectories(df):
    if 'x_ecef' not in df.columns or 'y_ecef' not in df.columns or 'z_ecef' not in df.columns:
        print("No hay coordenadas ECEF para visualizar.")
        return
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    valid = df['x_ecef'].notna() & df['y_ecef'].notna() & df['z_ecef'].notna()
    ax.scatter(df.loc[valid, 'x_ecef'], df.loc[valid, 'y_ecef'], df.loc[valid, 'z_ecef'],
               c='blue', s=1, label='Trayectorias')
    ax.set_xlabel('X (ECEF)')
    ax.set_ylabel('Y (ECEF)')
    ax.set_zlabel('Z (ECEF)')
    plt.title('Trayectorias de Aeronaves')
    plt.legend()
    plt.show()

# Pipeline principal
def process_aircraft_data(filepath, table_name='flight_data2'):
    df = load_data(filepath, table_name)
    df = clean_data(df)
    df = interpolate_missing_altitude(df)
    df = filter_outliers(df)
    df = filter_ground_aircraft(df)
    df = calculate_dynamics(df)
    df = transform_all_coordinates(df)
    metrics = calculate_metrics(df)
    visualize_trajectories(df)
    return df, metrics

# Ejecución
if __name__ == "__main__":
    filepath = "/content/Data2.db"  # Ruta al archivo .db
    try:
        # Se usa el nombre correcto de la tabla: 'flight_data2'
        processed_df, metrics = process_aircraft_data(filepath, table_name='flight_data2')
        processed_df.to_csv("processed_aircraft_data.csv", index=False)
        print("Procesamiento completado y datos guardados en 'processed_aircraft_data.csv'")
    except Exception as e:
        print(f"Error al procesar los datos: {e}")

"""Interpretación de los datos procesados de 'flight_data2'
El análisis de 'flight_data2' revela un volumen significativamente mayor de datos en comparación con conjuntos anteriores, con 22,974 coordenadas transformadas al sistema ECEF (Earth-Centered, Earth-Fixed). Esto sugiere vuelos de mayor duración, mayor densidad de registros o un monitoreo más detallado. Sin embargo, también se evidencia una mayor cantidad de datos faltantes, con 122 interpolaciones de altitud, lo cual supera las 88 del análisis previo. Esto podría atribuirse a variabilidad en las mediciones o a fallas puntuales en los sensores de las aeronaves. Además, se descartaron 2,457 registros con altitudes menores a 100 m, lo que refuerza la limpieza de datos para excluir fases como despegue o rodaje. El filtrado de 20 outliers asegura que las altitudes sean representativas de fases operativas. La consistencia en altitudes por encima de los 100 m es un indicador de un procesamiento robusto y alineado con operaciones aéreas reales.


En cuanto a la dinámica operacional, la tasa vertical media de 2.87 m/s es consistente con operaciones comerciales típicas, similares a las observadas en análisis previos (2.91 m/s). No obstante, el percentil 95 del tiempo entre contactos, con un valor de 25,597 segundos (~7.1 horas), es un indicador inusual para vuelos comerciales tradicionales. Esto podría asociarse a vuelos transoceánicos con intervalos más largos entre actualizaciones satelitales, aeronaves de larga autonomía como drones estratégicos o incluso errores en la sincronización de registros. También podría reflejar operaciones en regiones remotas donde la tecnología de comunicación es limitada. Este intervalo extenso requiere una validación exhaustiva para descartar inconsistencias o confirmar contextos operativos especializados, como misiones de vigilancia prolongada. En conclusión, 'flight_data2' representa un conjunto de datos más complejo y amplio, que destaca por su consistencia en altitudes y su procesamiento meticuloso, pero que plantea preguntas sobre la escala temporal y las anomalías en el tiempo entre contactos, las cuales merecen mayor análisis.
"""

!pip install stable-baselines3[extra]
!pip install 'shimmy>=2.0'

"""##Evaluación de Algoritmos de Reinforcement Learning para Control de Aeronaves


Basado en los datos procesados y los requisitos del sistema de control de aeronaves, se evaluarán distintos algoritmos de RL.

Evaluación preliminar:
  
1. PPO (Proximal Policy Optimization)
   - Fortalezas:
     • Maneja acciones continuas.
     • Estabilidad garantizada por clipping.
     • Eficiente en uso de datos.
   - Aplicación Recomendada:
     • Control en tiempo real de parámetros como drag_force y angular_accel.

2. SAC (Soft Actor-Critic)
   - Fortalezas:
     • Maximiza la entropía favoreciendo exploración.
     • Efectivo en entornos estocásticos.
   - Aplicación Recomendada:
     • Optimización multi-objetivo, equilibrio entre velocidad y consumo de combustible.

3. DQN (Deep Q-Network)
   - No recomendado para este caso, por estar enfocado en acciones discretas.

4. A3C (Asynchronous Advantage Actor-Critic)
   - Fortalezas:
     • Entrenamiento paralelo (se utiliza A2C como aproximación).
   - Requiere recursos elevados.

5. DDPG (Deep Deterministic Policy Gradient)
   - Fortalezas:
     • Adecuado para acciones continuas.
   - Riesgos:
     • Sensible a los hiperparámetros.

Recomendación Final:
- Mejor opción: PPO por su equilibrio entre estabilidad y control continuo.
- Alternativa: SAC para escenarios con alta incertidumbre.
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from stable_baselines3 import PPO, SAC, DDPG, A2C, TD3
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_checker import check_env
from typing import Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# 1. Entorno mejorado (ahora basado en gymnasium.Env)

class AircraftControlEnv(gym.Env):
    """Entorno de control de aeronave con física simplificada.

    Args:
        data (pd.DataFrame): Datos de vuelo con columnas requeridas.
        target_velocity (float, optional): Velocidad objetivo. Default 200.0.
        max_steps (int, optional): Máximo de pasos por episodio. Si None, usa todo el dataset.
    """

    metadata = {"render.modes": ["human"]}

    def __init__(self, data: pd.DataFrame, target_velocity: float = 200.0,
                 max_steps: Optional[int] = None):
        super(AircraftControlEnv, self).__init__()

        # 1.1. Validación de datos de entrada
        required_columns = ['velocity', 'heading', 'drag_force', 'angular_accel']
        if not all(col in data.columns for col in required_columns):
            missing = set(required_columns) - set(data.columns)
            raise ValueError(f"Datos faltantes: {missing}")

        # 1.2. Configuración de espacios de acción y observación
        self.action_space = spaces.Box(
            low=np.array([-10.0, -15.0], dtype=np.float32),
            high=np.array([10.0, 15.0], dtype=np.float32),
            dtype=np.float32
        )

        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(4,),
            dtype=np.float32
        )

        # 1.3. Procesamiento de datos mejorado
        self.clean_data = self._preprocess_data(data[required_columns].copy())

        # 1.4. Entrenar modelos físicos
        self._train_models(self.clean_data)

        # 1.5. Configuración de simulación
        self.data = self.clean_data.reset_index(drop=True)
        self.current_step = 0
        self.max_steps = min(max_steps, len(self.data) - 1) if max_steps else len(self.data) - 1
        self.target_velocity = target_velocity
        self.safe_velocity = [50.0, 350.0]  # Rango de velocidad segura
        self.episode_reward = 0.0

    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Limpieza y normalización de datos."""
        # Pipeline de preprocesamiento
        preprocessor = Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ])

        # Aplicar solo a features, no a targets
        features = ['velocity', 'heading']
        targets = ['drag_force', 'angular_accel']

        # Eliminar filas con targets faltantes
        clean_data = data.dropna(subset=targets).copy()

        # Imputar y escalar features
        clean_data[features] = preprocessor.fit_transform(clean_data[features])

        return clean_data

    def _train_models(self, clean_data: pd.DataFrame):
        """Entrenar modelos de física con validación."""
        try:
            # Modelo de fuerza de arrastre: f(v) = a*v + b*v²
            X_drag = np.column_stack([clean_data['velocity'], clean_data['velocity']**2])
            y_drag = clean_data['drag_force'].values
            self.drag_model = LinearRegression().fit(X_drag, y_drag)

            # Modelo de aceleración angular: f(v, sin(θ), cos(θ))
            X_angular = np.column_stack([
                clean_data['velocity'],
                np.sin(np.radians(clean_data['heading'])),
                np.cos(np.radians(clean_data['heading']))
            ])
            y_angular = clean_data['angular_accel'].values
            self.angular_model = LinearRegression().fit(X_angular, y_angular)

            # Validar modelos
            if (self.drag_model.score(X_drag, y_drag) < 0.5 or
                self.angular_model.score(X_angular, y_angular) < 0.5):
                warnings.warn("Los modelos físicos tienen bajo R². Considere modelos más complejos.")

        except Exception as e:
            raise RuntimeError(f"Error al entrenar modelos físicos: {str(e)}")

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """
        Ejecuta un paso de simulación.

        Args:
            action: Array de 2 elementos [Δvelocidad, Δrumbo]

        Returns:
            Tuple: (observación, recompensa, terminado, truncado, info)
        """
        # Validar acción
        if not self.action_space.contains(action):
            action = np.clip(action, self.action_space.low, self.action_space.high)
            warnings.warn(f"Acción {action} fuera de límites. Recortando.")

        current_state = self.data.iloc[self.current_step]

        # Aplicar acción con ruido gaussiano
        applied_action = action + np.random.normal(scale=0.1, size=2)

        # Actualizar estado
        new_velocity = current_state['velocity'] + applied_action[0]
        new_heading = (current_state['heading'] + applied_action[1]) % 360

        # Calcular efectos físicos
        drag_force = self._calculate_drag(new_velocity)
        angular_accel = self._calculate_angular_accel(new_velocity, new_heading)

        # Calcular recompensa
        reward = self._calculate_reward(
            new_velocity, new_heading, drag_force, angular_accel, applied_action
        )
        self.episode_reward += reward

        # Verificar condición de terminación
        terminated = (self.current_step >= self.max_steps or
                      not self.safe_velocity[0] <= new_velocity <= self.safe_velocity[1])
        # En este ejemplo, no usamos truncamiento por límite de tiempo
        truncated = False

        # Preparar observación
        obs = np.array([new_velocity, new_heading, drag_force, angular_accel], dtype=np.float32)

        # Información adicional
        info = {
            'velocity': new_velocity,
            'heading': new_heading,
            'drag_force': drag_force,
            'angular_accel': angular_accel,
            'episode_reward': self.episode_reward
        }

        self.current_step += 1

        return obs, reward, terminated, truncated, info

    def _calculate_drag(self, velocity: float) -> float:
        """Calcular fuerza de arrastre."""
        X_drag = np.array([[velocity, velocity**2]])
        return float(self.drag_model.predict(X_drag)[0])

    def _calculate_angular_accel(self, velocity: float, heading: float) -> float:
        """Calcular aceleración angular."""
        X_angular = np.array([
            velocity,
            np.sin(np.radians(heading)),
            np.cos(np.radians(heading))
        ]).reshape(1, -1)
        return float(self.angular_model.predict(X_angular)[0])

    def _calculate_reward(self, velocity: float, heading: float,
                            drag_force: float, angular_accel: float,
                            action: np.ndarray) -> float:
        """Función de recompensa mejorada."""
        # Penalización por desviación de la velocidad objetivo
        velocity_error = np.clip(velocity - self.target_velocity, -100, 100)
        velocity_penalty = 0.1 * (velocity_error**2)

        # Penalización por fuerza de arrastre (eficiencia energética)
        drag_penalty = 0.01 * drag_force

        # Penalización por acciones grandes (suavidad de control)
        action_penalty = 0.001 * np.sum(action**2)

        # Recompensa adicional por mantenerse en rumbo estable
        heading_stability = 0.05 * (1 - np.abs(np.sin(np.radians(heading))))

        reward = -velocity_penalty - drag_penalty - action_penalty + heading_stability
        return float(reward)

    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, dict]:
        """
        Reinicia el entorno al estado inicial.

        Returns:
            Tuple: (observación inicial, info)
        """
        self.current_step = 0
        self.episode_reward = 0.0
        current_state = self.data.iloc[self.current_step]

        obs = np.array([
            current_state['velocity'],
            current_state['heading'],
            current_state['drag_force'],
            current_state['angular_accel']
        ], dtype=np.float32)

        return obs, {}

    def render(self, mode: str = 'human'):
        """Visualización del estado actual (opcional)."""
        if mode == 'human':
            print(f"Step: {self.current_step}/{self.max_steps} | "
                  f"Velocity: {self.data.iloc[self.current_step]['velocity']:.1f} | "
                  f"Heading: {self.data.iloc[self.current_step]['heading']:.1f}° | "
                  f"Reward: {self.episode_reward:.2f}")


# 2. Función de entrenamiento mejorada

def train_and_evaluate_rl(env: gym.Env, algo: str = 'PPO',
                            total_timesteps: int = 30000) -> Tuple:
    """
    Entrena y evalúa un modelo RL.

    Args:
        env: Entorno basado en Gymnasium
        algo: Algoritmo a usar ('PPO', 'SAC', 'DDPG', 'A2C', 'DQN')
        total_timesteps: Pasos totales de entrenamiento

    Returns:
        Tuple: (modelo entrenado, recompensa media, std de recompensa)
    """
    # Validar entorno
    check_env(env)

    # Configuraciones por algoritmo
    algo_configs = {
        'PPO': {
            'class': PPO,
            'params': {
            'learning_rate': 3e-4,
            'batch_size': 256,
            'n_steps': 2048,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'clip_range': 0.2
            }
        },
        'SAC': {
            'class': SAC,
            'params': {
            'learning_rate': 3e-4,
            'batch_size': 512,
            'buffer_size': 1_000_000,
            'tau': 0.005,
            'gamma': 0.99
            }
        },
        'DDPG': {
            'class': DDPG,
            'params': {
            'learning_rate': 1e-3,
            'batch_size': 128,
            'buffer_size': 1_000_000,
            'tau': 0.005,
            'gamma': 0.99
            }
        },
        'A2C': {
            'class': A2C,
            'params': {
            'learning_rate': 2e-4,
            'n_steps': 5,
            'gamma': 0.99
            }
        },
        'DQN': {
            'class': DQN,
            'params': {
            'learning_rate': 1e-4,
            'batch_size': 32,
            'buffer_size': 1_000_000,
            'tau': 0.005,
            'gamma': 0.99
            }
        }
    }

    config = algo_configs.get(algo.upper())
    if not config:
        raise ValueError(f"Algoritmo no soportado: {algo}. Opciones: {list(algo_configs.keys())}")

    try:
        # Crear y entrenar el modelo
        model = config['class']("MlpPolicy", env, verbose=1, **config['params'])
        model.learn(total_timesteps=total_timesteps, progress_bar=True)

        # Evaluación
        mean_reward, std_reward = evaluate_policy(
            model, env, n_eval_episodes=10, deterministic=True
        )

        return model, mean_reward, std_reward

    except Exception as e:
        raise RuntimeError(f"Error en entrenamiento con {algo}: {str(e)}")

# 3. Ejecución principal mejorada

def main():
    """Función principal de ejecución."""
    try:
        # Cargar y validar datos
        raw_data = pd.read_csv("processed_aircraft_data.csv")
        print(f"Datos cargados: {len(raw_data)} registros")

        # Configuración de experimento
        algorithms = ['PPO', 'SAC', 'DDPG', 'A2C', 'DQN']
        results = {}

        # Entrenar y evaluar cada algoritmo
        for algo in algorithms:
            print(f"\n=== Entrenando con {algo} ===")
            env = AircraftControlEnv(raw_data, target_velocity=200.0, max_steps=1000)

            try:
                model, mean_reward, std_reward = train_and_evaluate_rl(
                    env, algo, total_timesteps=50000
                )
                results[algo] = (mean_reward, std_reward)
                print(f"Resultado {algo}: {mean_reward:.2f} ± {std_reward:.2f}")

                # Guardar modelo (opcional)
                model.save(f"aircraft_control_{algo.lower()}")

            except Exception as e:
                print(f"Error con {algo}: {str(e)}")
                results[algo] = (None, None)

            finally:
                env.close()

        # Reporte de resultados
        print("\n=== Resultados Finales ===")
        print("| Algoritmo | Recompensa Media | Desviación Estándar |")
        print("|-----------|------------------|---------------------|")
        for algo, (mean, std) in results.items():
            mean_str = f"{mean:.2f}" if mean is not None else "Error"
            std_str = f"{std:.2f}" if std is not None else "N/A"
            print(f"| {algo:9} | {mean_str:16} | {std_str:19} |")

    except Exception as e:
        print(f"\nError crítico: {str(e)}")
        raise

if __name__ == "__main__":
    main()

"""La tabla presentada muestra los resultados de la evaluación de cinco algoritmos de aprendizaje por refuerzo en términos de su recompensa media y desviación estándar. Estos valores permiten analizar tanto el rendimiento promedio de cada algoritmo como la consistencia de sus resultados.

El algoritmo Proximal Policy Optimization (PPO) destaca por su rendimiento estable, con una recompensa media de -1001.54 y una desviación estándar de 1.68. Esto indica que el algoritmo no solo logra un desempeño cercano al óptimo en promedio, sino que también presenta una variabilidad mínima en sus resultados, lo que lo hace confiable para tareas donde la estabilidad es crucial.

De manera similar, el algoritmo Advantage Actor-Critic (A2C) muestra un rendimiento competitivo, con una recompensa media de -1006.41 y una desviación estándar de 4.24. Aunque su desempeño promedio es ligeramente inferior al de PPO, sigue siendo un algoritmo estable, con una variabilidad baja en comparación con otros métodos.

En contraste, los algoritmos Soft Actor-Critic (SAC) y Deep Deterministic Policy Gradient (DDPG) presentan un rendimiento significativamente inferior. SAC tiene una recompensa media de -18187.19 y una desviación estándar de 191.33, mientras que DDPG alcanza una recompensa media de -5678.85 con una desviación estándar de 86.93. Estos resultados reflejan no solo un desempeño promedio más bajo, sino también una mayor variabilidad en los resultados, lo que sugiere que estos algoritmos son menos consistentes y menos adecuados para entornos donde la estabilidad es importante.

Finalmente, el algoritmo Deep Q-Network (DQN) muestra el peor desempeño general, con una recompensa media de -18278.45 y una desviación estándar de 221.77. Esto indica que, además de obtener resultados promedio muy bajos, DQN es el algoritmo con mayor variabilidad, lo que lo hace menos confiable en aplicaciones prácticas.

En resumen, la tabla evidencia que los algoritmos PPO y A2C son los más estables y confiables, mientras que SAC, DDPG y DQN presentan un rendimiento inferior y una mayor variabilidad. Estos resultados subrayan la importancia de seleccionar el algoritmo adecuado según las características del problema y los objetivos específicos, priorizando la estabilidad o el rendimiento según sea necesario.
"""

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns

# Configurar figura y estructura de grid
fig = plt.figure(figsize=(20, 15))
gs = gridspec.GridSpec(2, 2, figure=fig, height_ratios=[3, 1], hspace=1.5, wspace=0.3)

# Función para crear tabla de porcentajes mejorada
def crear_tabla(ax, datos_columna, subplot_pos):
    # Calcular total y recolectar datos
    total = len(datos_columna)
    bin_data = []

    # Recolectar datos de los bins
    for p in ax.patches:
        altura = p.get_height()
        porcentaje_valor = 100 * altura / total
        inicio_bin = p.get_x()
        fin_bin = p.get_x() + p.get_width()
        bin_data.append((inicio_bin, fin_bin, porcentaje_valor))

    # Filtrar y agrupar datos
    filtered_data = []
    sum_otros = 0.0

    for inicio, fin, pct in bin_data:
        if pct >= 0.8:
            filtered_data.append([f"{inicio:.1f}-{fin:.1f}", f"{pct:.1f}%"])
        else:
            sum_otros += pct

    # Agregar fila de Otros si es necesario
    if sum_otros > 0:
        filtered_data.append(["Otros", f"{sum_otros:.1f}%"])

    # Crear subplot para la tabla
    ax_tabla = fig.add_subplot(gs[1, subplot_pos])
    ax_tabla.axis('off')

    # Crear tabla con estilo mejorado
    tabla = ax_tabla.table(
        cellText=filtered_data,
        colLabels=['Rango', 'Porcentaje'],
        loc='center',
        cellLoc='center',
        colColours=['#f0f0f0', '#f0f0f0']
    )

    # Formatear tabla
    tabla.auto_set_column_width([0, 1])
    tabla.set_fontsize(12)
    tabla.scale(1, 1.8)
    for (row, col), cell in tabla.get_celld().items():
        if row == 0:  # Encabezados
            cell.set_text_props(weight='bold', color='black')
            cell.set_facecolor('#404040')

# Gráfico para Velocity
ax1 = fig.add_subplot(gs[0, 0])
sns.histplot(datos['velocity'], kde=True, color='skyblue', ax=ax1)
ax1.set_title('Distribución de Velocity', pad=20)
crear_tabla(ax1, datos['velocity'], 0)

# Gráfico para Heading
ax2 = fig.add_subplot(gs[0, 1])
sns.histplot(datos['heading'], kde=True, color='dodgerblue', ax=ax2)
ax2.set_title('Distribución de Heading', pad=20)
crear_tabla(ax2, datos['heading'], 1)

plt.tight_layout()
plt.show()

"""Distribución de Velocidades
La distribución de velocidades revela un comportamiento bimodal, con un rango más frecuente de 229.9–245.2 m/s, que representa el 14.2% de los datos. Un segundo pico significativo se encuentra en el rango de 214.6–229.9 m/s, con un 12.5%. En general, las velocidades altas (superiores a 200 m/s) concentran un notable 34.5% de los datos, lo que sugiere que una gran proporción de los objetos analizados se mueve rápidamente, posiblemente en un contexto de vuelo. Por otro lado, las velocidades bajas, entre 0.0 y 15.3 m/s, son menos comunes, representando solo el 9.5% de los registros. Esto podría indicar que los objetos en reposo o en movimiento lento son menos frecuentes en este conjunto de datos. Es importante destacar que las unidades asumidas son metros por segundo (m/s), típicas de análisis de trayectorias aéreas.

Distribución de Rumbo

La distribución de rumbo es más uniforme, con un rango más frecuente de 56.8–75.8 grados, que abarca el 7.2% de los datos. Otros picos menores se observan en los rangos de 246.2–265.2 grados (5.6%) y 284.1–303.0 grados (4.9%), lo que podría reflejar direcciones preferidas en ciertas rutas. La ausencia de un rumbo dominante sugiere operaciones en múltiples direcciones, posiblemente en un espacio aéreo congestionado o con rutas diversificadas. Además, los rangos de 0–180 grados y 180–360 grados están equilibrados, indicando que no hay preferencias geográficas claras. Para una interpretación más precisa, sería útil ajustar los rangos para incluir el valor de 360 grados, completando así el círculo.

Conclusión

Estos resultados destacan patrones clave en el comportamiento de los objetos analizados, con velocidades altas predominantes y una distribución de rumbo relativamente uniforme. Esto podría ser útil para optimizar rutas o validar modelos de movimiento en sistemas dinámicos.






"""

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns

# Configurar figura y estructura de grid
fig = plt.figure(figsize=(20, 15))
gs = gridspec.GridSpec(2, 2, figure=fig, height_ratios=[3, 1], hspace=1.5, wspace=0.3)

# Función para crear tabla de porcentajes mejorada
def crear_tabla(ax, datos_columna, subplot_pos):
    # Calcular total y recolectar datos
    total = len(datos_columna)
    bin_data = []

    # Recolectar datos de los bins
    for p in ax.patches:
        altura = p.get_height()
        porcentaje_valor = 100 * altura / total
        inicio_bin = p.get_x()
        fin_bin = p.get_x() + p.get_width()
        bin_data.append((inicio_bin, fin_bin, porcentaje_valor))

    # Filtrar y agrupar datos
    filtered_data = []
    sum_otros = 0.0

    for inicio, fin, pct in bin_data:
        if pct >= 0.9:
            filtered_data.append([f"{inicio:.1f}-{fin:.1f}", f"{pct:.1f}%"])
        else:
            sum_otros += pct

    # Agregar fila de Otros si es necesario
    if sum_otros > 0:
        filtered_data.append(["Otros", f"{sum_otros:.1f}%"])

    # Crear subplot para la tabla
    ax_tabla = fig.add_subplot(gs[1, subplot_pos])
    ax_tabla.axis('off')

    # Crear tabla con estilo mejorado
    tabla = ax_tabla.table(
        cellText=filtered_data,
        colLabels=['Rango', 'Porcentaje'],
        loc='center',
        cellLoc='center',
        colColours=['#f0f0f0', '#f0f0f0']
    )

    # Formatear tabla
    tabla.auto_set_column_width([0, 1])
    tabla.set_fontsize(12)
    tabla.scale(1, 1.8)
    for (row, col), cell in tabla.get_celld().items():
        if row == 0:  # Encabezados
            cell.set_text_props(weight='bold', color='black')
            cell.set_facecolor('#404040')

# Gráfico para baroaltitude
ax1 = fig.add_subplot(gs[0, 0])
sns.histplot(datos['baro_altitude'], kde=True, color='skyblue', ax=ax1)
ax1.set_title('Distribución de baroaltitude', pad=20)
crear_tabla(ax1, datos['baro_altitude'], 0)

# Gráfico para Heading
ax2 = fig.add_subplot(gs[0, 1])
sns.histplot(datos['geo_altitude'], kde=True, color='dodgerblue', ax=ax2)
ax2.set_title('Distribución de geoaltitude', pad=20)
crear_tabla(ax2, datos['geo_altitude'], 1)

plt.tight_layout()
plt.show()

"""Distribución de Baroaltitude (Altitud Barométrica)

La altitud barométrica muestra un rango de valores entre aproximadamente -114 y 17,500 metros. Las concentraciones más significativas se encuentran en los rangos de 10,237 a 12,116 metros, representando el 17.6% de las observaciones, y de 11,176.6 a 12,153.9 metros, con un 15.4%. Estas alturas coinciden con las altitudes típicas de crucero de vuelos comerciales. Las observaciones negativas podrían derivarse de errores de calibración en sensores o de condiciones específicas del vuelo, como el despegue o aterrizaje, donde la presión atmosférica puede afectar las mediciones.

Distribución de Geoaltitude (Altitud Geográfica)

La altitud geográfica varía entre aproximadamente -61 y 19,000 metros. El rango más frecuente es 10,736.6 a 11,635.7 metros, representando un 20.1% de las observaciones. En comparación con la baroaltitude, la geoaltitude suele registrar valores ligeramente más altos, lo cual es coherente con el uso del sistema ECEF (Earth-Centered, Earth-Fixed), que mide altitudes en relación con un modelo geométrico de la Tierra.

Consideraciones Operacionales y Calidad de Datos

Las altitudes más frecuentes, entre 10,000 y 12,000 metros, reflejan operaciones normales de vuelos comerciales en altitud de crucero. La baja proporción de valores categorizados como "Otros" (apenas 1.1% en baroaltitude y 1.2% en geoaltitude) sugiere un preprocesamiento efectivo y una mínima presencia de datos atípicos. Además, la tasa de ascenso/descenso promedio de 2.91 m/s indica maniobras suaves y consistentes con vuelos regulares.
En conjunto, estas distribuciones permiten entender mejor el comportamiento operacional y la calidad de los datos recopilados.

"""

sns.jointplot(x='velocity', y='baro_altitude', data=datos.sample(5000), kind='scatter')
plt.show()

"""Relación entre velocidad y altitud (gráfico central)
Se observa una correlación positiva clara: a medida que la altitud aumenta, también lo hace la velocidad.

En los niveles bajos de altitud (0 a ~2000 m), hay una gran dispersión de velocidades, incluyendo valores muy bajos, lo que probablemente corresponde a fases de despegue y aterrizaje.

Entre 10,000 y 13,000 metros se concentra una gran cantidad de puntos con velocidades entre 200 y 300 km/h (o nudos, según el sistema usado), lo que es típico de vuelos de crucero en aviación comercial.

Hay una cierta “estratificación” o acumulación de puntos a alturas específicas, lo que podría estar relacionado con altitudes estándar de crucero (como FL350 = 35,000 pies ≈ 10,668 m).

Histograma de velocidad (parte superior)
La mayoría de los valores están entre 200 y 300, lo cual coincide con las velocidades comunes en fase de crucero.

Hay una cantidad notable de vuelos con velocidades inferiores a 100, lo que también sugiere maniobras en tierra o etapas iniciales/finales del vuelo.

Histograma de altitud barométrica (parte derecha)
Confirma los picos en los rangos de altitud ya observados anteriormente (~10,000 a 12,000 m).

También se observan agrupamientos en altitudes menores (<2000 m), coherente con fases de ascenso o descenso.

Conclusión operacional
Este gráfico describe claramente las fases del vuelo:

Baja altitud + baja velocidad → despegue/aterrizaje.

Alta altitud + alta velocidad → crucero.

La correlación positiva sugiere que los datos son coherentes y están bien calibrados.

No se detectan valores extremadamente atípicos, lo que apoya la calidad del preprocesamiento de datos.

## 1. Trayectoria del Vuelo en Coordenadas Geográficas
"""

import geopandas as gpd
from shapely.geometry import Point

# Crear geometrías de puntos a partir de latitud y longitud
geometry = [Point(xy) for xy in zip(datos['longitude'], datos['latitude'])]
geo_df = gpd.GeoDataFrame(datos, geometry=geometry)

# Configurar el sistema de referencia geográfico (WGS84)
geo_df.set_crs(epsg=4326, inplace=True)

# Plotear la trayectoria del vuelo
fig, ax = plt.subplots(figsize=(10, 10))
geo_df.plot(ax=ax, marker='o', color='blue', markersize=1, label='Trayectoria')
plt.title('Trayectoria del Vuelo')
plt.xlabel('Longitud')
plt.ylabel('Latitud')
plt.legend()
plt.grid(True)
plt.show()

"""## 2. Datos de resultados de algoritmos (ejemplo de tabla final)"""

import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib  # Para guardar los modelos
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

# 1. Datos de resultados de algoritmos (ejemplo de tabla final)
algoritmos = ['PPO', 'SAC', 'DDPG', 'A2C', 'DQN']
recompensas_medias = [-1001.54, -18187.19, -5678.85, -1006.41, -18278.45]
desviaciones_estandar = [1.68, 191.33, 86.93, 4.24, 221.77]


# Gráfico de comparación de algoritmos
plt.figure(figsize=(10, 6))
plt.bar(algoritmos, recompensas_medias, yerr=desviaciones_estandar, capsize=5, color='darkblue', edgecolor='black')
plt.xlabel('Algoritmo')
plt.ylabel('Recompensa Media')
plt.title('Comparación de Algoritmos (Menor valor negativo = mejor desempeño)')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig("comparacion_algoritmos.png", dpi=300)
plt.show()

def save_processed_data_to_db(df, db_path='Data2.db', table_name='processed_flight_data'):
    with sqlite3.connect(db_path) as conn:
        df.to_sql(table_name, conn, if_exists='replace', index=False)
        print(f"Datos procesados guardados en la tabla '{table_name}' de la base de datos.")

save_processed_data_to_db(processed_df)

df = load_data(filepath, table_name='processed_flight_data')

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import joblib

columnas_interes = ['velocity', 'heading', 'baro_altitude', 'geo_altitude']
df1 = df.dropna(subset=columnas_interes)  # Eliminar registros faltantes
X = df1[columnas_interes].values

# Normalizar las características
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Nombres de las columnas
column_names = ['velocity', 'heading', 'baro_altitude', 'geo_altitude']

# Guardar el modelo PCA
joblib.dump(pca, 'pca_model.pkl')
print("Modelo PCA guardado en 'pca_model.pkl'.")

# Calcular el 5% de los datos
num_samples = X_pca.shape[0]
num_samples_to_plot = int(num_samples * 0.05)  # 5% de las muestras se puede ajustar si quiere

# Crear el biplot
plt.figure(figsize=(8, 6))

# Graficar solo el 5% de los datos proyectados
plt.scatter(X_pca[:num_samples_to_plot, 0], X_pca[:num_samples_to_plot, 1], c='blue', alpha=0.5, edgecolor='k', label='Datos')

# Añadir las flechas (vectores de las variables originales)
for i, (comp1, comp2) in enumerate(zip(pca.components_[0], pca.components_[1])):
    plt.arrow(0, 0, comp1, comp2, color='red', alpha=0.8, head_width=0.05, head_length=0.05)
    plt.text(comp1 * 1.15, comp2 * 1.15, column_names[i], color='red', ha='center', va='center')


# Configurar el gráfico
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.title('PCA - 5% de los Datos Proyectados')
plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)
plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()

# Guardar y mostrar el gráfico
plt.savefig("pca_5_percent.png", dpi=300)
plt.show()

"""**Partes principales del Biplot**

Puntos azules: Representan los datos proyectados sobre las dos primeras componentes principales (PC1 y PC2). Estos puntos muestran patrones o distribuciones en el nuevo espacio reducido. Solo se graficó el 5% de los datos para evitar saturación.

Flechas rojas: Indican cómo las variables originales contribuyen a PC1 y PC2. Su dirección y longitud reflejan la correlación entre variables y su influencia en el nuevo espacio.

Ejes PC1 y PC2: PC1 explica la mayor varianza, mientras que PC2 es ortogonal y explica la siguiente mayor parte de la variabilidad.



**Interpretación de las Flechas**

Variables correlacionadas: Flechas en la misma dirección (como baro_altitude y geo_altitude) sugieren alta correlación.

Variables independientes: Flechas ortogonales (90°) reflejan poca o nula correlación, como podría ser el caso de heading y las altitudes.

Correlación negativa: Flechas en direcciones opuestas (ángulo > 90°) indican relación inversa.


**Distribución de los Puntos**

Los puntos azules pueden mostrar patrones como clusters (grupos separados) o una dispersión uniforme. Por ejemplo, clusters podrían reflejar diferencias entre fases del vuelo (despegue, crucero, etc.), mientras que una dispersión uniforme sugiere datos sin agrupamientos claros.


**Conclusión Operacional**


PC1 y PC2 explican la mayor parte de la variabilidad en los datos.
Las flechas reflejan correlaciones clave: por ejemplo, la alta relación entre baro_altitude y geo_altitude, o la independencia de heading respecto a las demás variables.

El gráfico ayuda a identificar subgrupos de datos y patrones relevantes para análisis posteriores, como segmentar las fases del vuelo o evaluar correlaciones específicas por contexto.

## **GUARDAR Y USAR Stable Baselines3**
"""

import gym
from stable_baselines3 import PPO, SAC, DDPG, A2C, DQN
from stable_baselines3.common.env_util import make_vec_env

# Diccionario de configuración de algoritmos
algo_configs = {
    'SAC': {
        'class': SAC,
        'params': {
            'learning_rate': 3e-4,
            'batch_size': 512,
            'buffer_size': 1_000_000,
            'tau': 0.005,
            'gamma': 0.99
        }
    },
    'DDPG': {
        'class': DDPG,
        'params': {
            'learning_rate': 1e-3,
            'batch_size': 128,
            'buffer_size': 1_000_000,
            'tau': 0.005,
            'gamma': 0.99
        }
    },
    'PPO': {
        'class': PPO,
        'params': {
            'learning_rate': 3e-4,
            'batch_size': 256,
            'n_steps': 2048,
            'gamma': 0.99,
            'gae_lambda': 0.95,
            'clip_range': 0.2
        }
    },
    'A2C': {
        'class': A2C,
        'params': {
            'learning_rate': 2e-4,
            'n_steps': 5,
            'gamma': 0.99
        }
    },
    'DQN': {
        'class': DQN,
        'params': {
            'learning_rate': 1e-4,
            'batch_size': 32,
            'buffer_size': 1_000_000,
            'tau': 0.005,
            'gamma': 0.99
        }
    }
}

#  Selecciona el algoritmo que quieres usar
algo_name = 'PPO'  # Cambia esto por: 'SAC', 'DDPG', 'A2C', 'DQN'

# Crea el entorno
env_id = 'CartPole-v1'
env = make_vec_env(env_id, n_envs=1)

# Instancia el modelo
algo_class = algo_configs[algo_name]['class']
algo_params = algo_configs[algo_name]['params']
model = algo_class("MlpPolicy", env, verbose=1, **algo_params)

#  Entrena el modelo
model.learn(total_timesteps=100_000)

#  Guarda el modelo
model.save(f"{algo_name.lower()}_{env_id.lower()}")

print(f"Modelo {algo_name} entrenado y guardado exitosamente en: {algo_name.lower()}_{env_id.lower()}")

"""# ¿Cómo cambiar de algoritmo o entorno?"""

algo_name = 'PPO'
env_id = 'LunarLander-v2'

"""El siguiente código en Python utiliza la biblioteca Stable Baselines3 para implementar y entrenar un modelo de aprendizaje por refuerzo en el entorno CartPole-v1. Se configuran varios algoritmos, incluyendo SAC, DDPG, PPO, A2C, y DQN, cada uno con sus respectivos parámetros de entrenamiento.

**Configuración de Algoritmos**

Se define un diccionario llamado algo_configs que contiene la configuración de cada algoritmo. Por ejemplo, el algoritmo PPO se configura con un learning rate de 3e-4, un tamaño de lote de 256, y un número de pasos de 2048. Cada algoritmo tiene parámetros específicos que optimizan su rendimiento en el entorno.

**Creación del Entorno**

El entorno se crea utilizando la función make_vec_env, que permite la creación de entornos vectorizados. En este caso, se utiliza el entorno CartPole-v1 con un solo entorno (n_envs=1).

**Entrenamiento del Modelo**

Una vez configurado el algoritmo y creado el entorno, se instancia el modelo correspondiente utilizando la clase del algoritmo seleccionado. Luego, se entrena el modelo durante 100,000 pasos de tiempo. Al finalizar el entrenamiento, el modelo se guarda en un archivo con un nombre que incluye el algoritmo y el entorno.

**Conclusión**

Este código proporciona una base sólida para experimentar con diferentes algoritmos de aprendizaje por refuerzo, permitiendo a los desarrolladores ajustar parámetros y evaluar el rendimiento en el entorno de CartPole-v1.

## Mejoras Avanzadas para Algoritmos de Aprendizaje por Refuerzo en Python

El aprendizaje por refuerzo presenta desafíos únicos en entornos con espacios de acción continuos, como el control de aeronaves. Tras analizar el código proporcionado, presento un conjunto de mejoras estructurales y optimizaciones para elevar el rendimiento de los algoritmos DQN y A3C, así como implementaciones adicionales específicamente diseñadas para espacios de acción continuos.

Implementación mejorada de discretización
"""

from gymnasium.spaces import Discrete
import itertools
import numpy as np
import pandas as pd
from typing import Optional, List, Tuple

class AircraftControlEnvDiscrete(AircraftControlEnv):
    def __init__(self, data: pd.DataFrame,
                 target_velocity: float = 200.0,
                 max_steps: Optional[int] = None,
                 vel_increments: List[float] = [-10, -5, 0, 5, 10],  # Mayor granularidad
                 hdg_increments: List[float] = [-15, -10, -5, 0, 5, 10, 15],
                 reward_shaping: bool = True):

        super().__init__(data, target_velocity, max_steps)

        # Configuración de acciones discretas
        self.vel_inc = vel_increments
        self.hdg_inc = hdg_increments
        self.discrete_actions = list(itertools.product(vel_increments, hdg_increments))

        # Espacio de acción discreto
        self.action_space = Discrete(len(self.discrete_actions))

        # Estadísticas para normalización online
        self.enable_reward_shaping = reward_shaping
        self.running_stats = RunningStats(self.observation_space.shape[0])
        self.prev_action = np.zeros(2, dtype=np.float32)

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:
        # Conversión robusta a acción continua con normalización
        vel_inc, hdg_inc = self.discrete_actions[action]
        continuous_action = np.array([
            vel_inc / max(abs(max(self.vel_inc)), abs(min(self.vel_inc))),
            hdg_inc / max(abs(max(self.hdg_inc)), abs(min(self.hdg_inc)))
        ], dtype=np.float32)

        # Ejecutar paso en el entorno base
        obs, reward, done, info = super().step(continuous_action)

        # Reward shaping para suavizar transiciones
        if self.enable_reward_shaping:
            action_diff = np.linalg.norm(continuous_action - self.prev_action)
            smoothness_reward = -0.1 * action_diff  # Penalizar cambios bruscos
            reward += smoothness_reward
            self.prev_action = continuous_action

        # Actualizar estadísticas para normalización
        self.running_stats.update(obs)
        normalized_obs = self.normalize_observation(obs)

        return normalized_obs, reward, done, info

    def reset(self) -> np.ndarray:
        obs = super().reset()
        self.prev_action = np.zeros(2, dtype=np.float32)
        return self.normalize_observation(obs)

    def normalize_observation(self, obs: np.ndarray) -> np.ndarray:
        """Normaliza observaciones usando estadísticas en ejecución"""
        if self.running_stats.n < 2:
            return obs
        return (obs - self.running_stats.mean) / (np.sqrt(self.running_stats.var) + 1e-8)

# Clase auxiliar para normalización online
class RunningStats:
    def __init__(self, shape: int):
        self.n = 0
        self.mean = np.zeros(shape)
        self.var = np.zeros(shape)
        self.std = np.zeros(shape)

    def update(self, x: np.ndarray) -> None:
        self.n += 1
        if self.n == 1:
            self.mean = x
        else:
            old_mean = self.mean.copy()
            self.mean = old_mean + (x - old_mean) / self.n
            self.var = self.var + (x - old_mean) * (x - self.mean)
            if self.n > 1:
                self.std = np.sqrt(self.var / (self.n - 1))

"""## Optimización de A3C con Actor-Crítico
El código original de A3C presenta oportunidades para mejoras significativas, particularmente en la arquitectura actor-crítico y la estabilidad del entrenamiento:
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class ActorCritic(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int = 256,
                n_actions: int = None, continuous: bool = False,
                action_dim: int = None):
        super().__init__()
        self.continuous = continuous

        # Arquitectura base compartida con capas más amplias
        self.shared = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Red del actor adaptada al tipo de acción
        if continuous:
            assert action_dim is not None, "Se requiere action_dim para acciones continuas"
            self.actor_mean = nn.Linear(hidden_dim, action_dim)
            self.actor_logstd = nn.Parameter(torch.zeros(action_dim))
        else:
            assert n_actions is not None, "Se requiere n_actions para acciones discretas"
            self.actor = nn.Linear(hidden_dim, n_actions)

        # Red del crítico
        self.critic = nn.Linear(hidden_dim, 1)

        # Inicialización ortogonal para estabilidad
        self._initialize_weights()

    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
                nn.init.constant_(module.bias, 0)

    def forward(self, x):
        if isinstance(x, np.ndarray):
            x = torch.FloatTensor(x)

        x = self.shared(x)

        # Outputs dependiendo del tipo de acción
        if self.continuous:
            action_mean = self.actor_mean(x)
            action_logstd = self.actor_logstd.expand_as(action_mean)
            return action_mean, action_logstd, self.critic(x)
        else:
            return F.softmax(self.actor(x), dim=-1), self.critic(x)

    def get_action(self, state, deterministic=False):
        """Método unificado para obtener acciones"""
        if self.continuous:
            mean, logstd, _ = self(state)
            if deterministic:
                return mean.detach().numpy()
            std = torch.exp(logstd)
            dist = torch.distributions.Normal(mean, std)
            action = dist.sample()
            return action.detach().numpy()
        else:
            probs, _ = self(state)
            if deterministic:
                return torch.argmax(probs).item()
            dist = torch.distributions.Categorical(probs)
            action = dist.sample()
            return action.item()

"""# Implementación del Worker A3C con GAE"""

import multiprocessing as mp  # Importar el módulo multiprocessing
import numpy as np

class A3CWorker(mp.Process):
    def __init__(self, global_model, optimizer, env_fn, worker_id,
                 gamma=0.99, tau=0.95, max_steps=1000, update_interval=5,
                 continuous=False, entropy_coef=0.01):
        super().__init__()
        self.worker_id = worker_id
        self.env = env_fn()
        self.gamma = gamma
        self.tau = tau  # Parámetro lambda para GAE
        self.max_steps = max_steps
        self.update_interval = update_interval
        self.continuous = continuous
        self.entropy_coef = entropy_coef

        # Configurar modelos
        input_dim = self.env.observation_space.shape[0]
        if continuous:
            action_dim = self.env.action_space.shape[0]
            self.local_model = ActorCritic(input_dim, 256, continuous=True, action_dim=action_dim)
        else:
            n_actions = self.env.action_space.n
            self.local_model = ActorCritic(input_dim, 256, n_actions=n_actions, continuous=False)

        self.global_model = global_model
        self.optimizer = optimizer

        # Buffer para experiencias
        self.buffer = ExperienceBuffer()

    def compute_gae(self, next_value, rewards, masks, values):
        """Implementación de Generalized Advantage Estimation (GAE)"""
        values = values + [next_value]
        gae = 0
        returns = []

        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]
            gae = delta + self.gamma * self.tau * masks[step] * gae
            returns.insert(0, gae + values[step])

        return returns

    def run(self):
        """Ciclo principal de entrenamiento del worker"""
        total_step = 0
        episode = 0

        while total_step < self.max_steps:
            episode += 1
            episode_reward = 0
            done = False
            state = self.env.reset()

            # Sincronizar con modelo global
            self.local_model.load_state_dict(self.global_model.state_dict())

            step = 0

            while not done and step < self.max_steps:
                step += 1
                total_step += 1

                # Recolectar experiencias
                for _ in range(self.update_interval):
                    # Obtener acción y valor
                    if self.continuous:
                        mean, logstd, value = self.local_model(state)
                        value = value.item()
                        std = torch.exp(logstd)
                        dist = torch.distributions.Normal(mean, std)
                        action = dist.sample()
                        action_log_prob = dist.log_prob(action).sum().item()
                        action = action.detach().numpy()
                    else:
                        probs, value = self.local_model(state)
                        value = value.item()
                        dist = torch.distributions.Categorical(probs)
                        action = dist.sample()
                        action_log_prob = dist.log_prob(action).item()
                        action = action.item()

                    # Ejecutar acción
                    next_state, reward, done, info = self.env.step(action)
                    episode_reward += reward

                    # Almacenar transición
                    self.buffer.add(state, action, reward, 1.0 - float(done), value, action_log_prob)

                    state = next_state
                    if done:
                        break

                # Calcular returns usando GAE
                if done:
                    next_value = 0.0
                else:
                    _, next_value = self.local_model(state) if not self.continuous else (None, self.local_model(state)[2])
                    next_value = next_value.item()

                returns = self.compute_gae(next_value, self.buffer.rewards,
                                          [1.0 - d for d in self.buffer.dones],
                                          self.buffer.values)

                # Convertir a tensores
                states = torch.FloatTensor(self.buffer.states)
                actions = torch.FloatTensor(self.buffer.actions) if self.continuous else torch.LongTensor(self.buffer.actions)
                returns = torch.FloatTensor(returns)

                # Calcular pérdida y actualizar
                loss = self._compute_loss(states, actions, returns)

                self.optimizer.zero_grad()
                loss.backward()

                # Clipping de gradientes para estabilidad
                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), 0.5)

                # Aplicar gradientes al modelo global
                for global_param, local_param in zip(self.global_model.parameters(),
                                                 self.local_model.parameters()):
                    if global_param.grad is None:
                        global_param._grad = local_param.grad

                self.optimizer.step()
                self.buffer.clear()
                self.local_model.load_state_dict(self.global_model.state_dict())

"""## Implementación de DDPG para Entornos Continuos

Para entornos con acciones continuas como el control de aeronaves, el algoritmo DDPG (Deep Deterministic Policy Gradient) ofrece ventajas significativas sobre la discretización:
"""

class Actor(nn.Module):
    """Red neuronal del actor para DDPG"""
    def __init__(self, state_dim, action_dim, max_action):
        super(Actor, self).__init__()

        self.l1 = nn.Linear(state_dim, 256)
        self.l2 = nn.Linear(256, 256)
        self.l3 = nn.Linear(256, action_dim)

        self.max_action = max_action
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                nn.init.constant_(m.bias, 0)

    def forward(self, state):
        a = F.relu(self.l1(state))
        a = F.relu(self.l2(a))
        return torch.tanh(self.l3(a)) * self.max_action

class Critic(nn.Module):
    """Red neuronal del crítico para DDPG"""
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()

        # Red Q1
        self.l1 = nn.Linear(state_dim + action_dim, 256)
        self.l2 = nn.Linear(256, 256)
        self.l3 = nn.Linear(256, 1)

        # Red Q2 (para estabilidad)
        self.l4 = nn.Linear(state_dim + action_dim, 256)
        self.l5 = nn.Linear(256, 256)
        self.l6 = nn.Linear(256, 1)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                nn.init.constant_(m.bias, 0)

    def forward(self, state, action):
        sa = torch.cat([state, action], 1)

        q1 = F.relu(self.l1(sa))
        q1 = F.relu(self.l2(q1))
        q1 = self.l3(q1)

        q2 = F.relu(self.l4(sa))
        q2 = F.relu(self.l5(q2))
        q2 = self.l6(q2)

        return q1, q2

    def Q1(self, state, action):
        sa = torch.cat([state, action], 1)

        q1 = F.relu(self.l1(sa))
        q1 = F.relu(self.l2(q1))
        q1 = self.l3(q1)

        return q1

"""## Implementación de PPO para Mayor Estabilidad
Proximal Policy Optimization (PPO) representa un equilibrio excelente entre simplicidad y rendimiento, especialmente útil para problemas de control:

"""

class PPO:
    """Implementación de PPO con soporte para acciones continuas y discretas"""
    def __init__(
        self,
        state_dim,
        hidden_dim=256,
        n_actions=None,
        continuous=False,
        action_dim=None,
        lr=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_coef=0.2,
        value_coef=0.5,
        entropy_coef=0.01,
        max_grad_norm=0.5,
        update_epochs=10,
        batch_size=64,
        device="cuda" if torch.cuda.is_available() else "cpu"
    ):
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_coef = clip_coef
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.update_epochs = update_epochs
        self.batch_size = batch_size
        self.device = device
        self.continuous = continuous

        # Crear red neuronal para actor-crítico
        self.network = PPONetwork(
            state_dim,
            hidden_dim,
            n_actions=n_actions,
            continuous=continuous,
            action_dim=action_dim
        ).to(device)

        # Optimizador
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

        # Memoria para experiencias
        self.memory = PPOMemory(batch_size)

    # Métodos de entrenamiento omitidos por brevedad

"""## Hiperparámetros Recomendados por Algoritmo

"""

# DQN Discretizado
dqn_params = {
    'lr': 5e-4,
    'batch_size': 64,
    'gamma': 0.99,
    'target_update': 1000,
    'epsilon_start': 1.0,
    'epsilon_end': 0.1,
    'epsilon_decay': 50000,
    'buffer_size': 100000
}

# A3C Mejorado
a3c_params = {
    'lr': 2e-4,
    'gamma': 0.99,
    'tau': 0.95,  # Lambda para GAE
    'workers': 8,
    'update_interval': 5,
    'entropy_coef': 0.01,
    'max_grad_norm': 0.5
}

# DDPG
ddpg_params = {
    'lr_actor': 1e-4,
    'lr_critic': 1e-3,
    'gamma': 0.99,
    'tau': 0.005,  # Soft update
    'buffer_size': 1000000,
    'batch_size': 256,
    'policy_noise': 0.2,
    'noise_clip': 0.5,
    'policy_delay': 2
}

# PPO
ppo_params = {
    'lr': 3e-4,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_coef': 0.2,
    'value_coef': 0.5,
    'entropy_coef': 0.01,
    'max_grad_norm': 0.5,
    'update_epochs': 10,
    'batch_size': 64
}

"""## Código de Monitorización y Evaluación
Un aspecto frecuentemente olvidado es la monitorización adecuada del entrenamiento. Este código facilita la visualización de rendimiento:
"""

from torch.utils.tensorboard import SummaryWriter
import time
import matplotlib.pyplot as plt

class TrainingMonitor:
    def __init__(self, log_dir="runs/aircraft_control"):
        self.writer = SummaryWriter(log_dir + "_" + time.strftime("%Y%m%d-%H%M%S"))
        self.episode_rewards = []
        self.episode_lengths = []
        self.evaluation_rewards = []

    def log_episode(self, episode, reward, length, losses=None):
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)

        # Registrar en TensorBoard
        self.writer.add_scalar("train/episode_reward", reward, episode)
        self.writer.add_scalar("train/episode_length", length, episode)

        if losses:
            for key, value in losses.items():
                self.writer.add_scalar(f"train/{key}", value, episode)

        # Imprimir progreso
        if episode % 10 == 0:
            print(f"Episodio {episode}, Recompensa: {reward:.2f}, Pasos: {length}")

    def log_evaluation(self, episode, rewards):
        avg_reward = sum(rewards) / len(rewards)
        self.evaluation_rewards.append(avg_reward)

        self.writer.add_scalar("eval/average_reward", avg_reward, episode)
        print(f"Evaluación en episodio {episode}: Recompensa media = {avg_reward:.2f}")

    def plot_training_progress(self, smooth_window=10):
        plt.figure(figsize=(12, 8))

        # Suavizado de recompensas
        smoothed_rewards = []
        for i in range(len(self.episode_rewards)):
            start = max(0, i - smooth_window)
            smoothed_rewards.append(sum(self.episode_rewards[start:i+1]) / (i - start + 1))

        plt.subplot(2, 1, 1)
        plt.plot(self.episode_rewards, alpha=0.3, label='Recompensas')
        plt.plot(smoothed_rewards, label='Recompensas suavizadas')
        plt.xlabel('Episodio')
        plt.ylabel('Recompensa')
        plt.legend()

        plt.subplot(2, 1, 2)
        plt.plot(self.episode_lengths)
        plt.xlabel('Episodio')
        plt.ylabel('Duración episodio')

        plt.tight_layout()
        plt.savefig('training_progress.png')
        plt.show()

!pip install torch gymnasium numpy matplotlib

import pandas as pd
import sqlite3
from gymnasium import Env
from gymnasium.spaces import Discrete, Box
import numpy as np
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import itertools
import multiprocessing as mp

# Definición de ExperienceBuffer
class ExperienceBuffer:
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.values = []
        self.action_log_probs = []

    def add(self, state, action, reward, done, value, action_log_prob):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.dones.append(done)
        self.values.append(value)
        self.action_log_probs.append(action_log_prob)

    def clear(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.values = []
        self.action_log_probs = []

# Simulación del entorno de control aeronáutico
class AircraftControlEnvDiscrete(Env):
    def __init__(self, data_csv, db_path, target_velocity=200.0):
        super(AircraftControlEnvDiscrete, self).__init__()

        # Cargar datos
        self.data = pd.read_csv(data_csv)  # Cargar datos procesados
        self.conn = sqlite3.connect(db_path)  # Conectar a la base de datos
        self.cursor = self.conn.cursor()

        # Definir las dimensiones de la observación (ejemplo)
        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)

        # Espacio de acción discreto (simulación de cambios en velocidad y rumbo)
        self.action_space = Discrete(25)  # 25 acciones posibles, por ejemplo

        self.target_velocity = target_velocity
        self.current_step = 0

    def step(self, action):
        # Simular una acción y calcular una recompensa (esto es un ejemplo)
        velocity_change = self.data['velocity'][self.current_step] + action  # Usando el 'action' para modificar velocidad
        reward = -abs(self.target_velocity - velocity_change)  # Penalizar según la diferencia con la velocidad objetivo

        # Obtener próxima observación
        next_obs = np.random.rand(10)  # Ejemplo de próxima observación aleatoria

        # Incrementar paso y verificar si es el final de un episodio
        self.current_step += 1
        done = self.current_step >= len(self.data)

        return next_obs, reward, done, {}

    def reset(self):
        # Restablecer el estado
        self.current_step = 0
        return np.random.rand(10)  # Devuelve una observación inicial aleatoria

    def close(self):
        # Cerrar la conexión a la base de datos
        self.conn.close()

# Ejemplo simple de ActorCritic (asegúrate de tener la clase definida, aquí se muestra una versión mínima)
class ActorCritic(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int = 256, n_actions: int = None, continuous: bool = False, action_dim: int = None):
        super().__init__()
        self.continuous = continuous
        self.shared = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        if continuous:
            assert action_dim is not None, "Se requiere action_dim para acciones continuas"
            self.actor_mean = nn.Linear(hidden_dim, action_dim)
            self.actor_logstd = nn.Parameter(torch.zeros(action_dim))
        else:
            assert n_actions is not None, "Se requiere n_actions para acciones discretas"
            self.actor = nn.Linear(hidden_dim, n_actions)

        self.critic = nn.Linear(hidden_dim, 1)
        self._initialize_weights()

    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
                nn.init.constant_(module.bias, 0)

    def forward(self, x):
        if isinstance(x, np.ndarray):
            x = torch.FloatTensor(x)
        x = self.shared(x)
        if self.continuous:
            action_mean = self.actor_mean(x)
            action_logstd = self.actor_logstd.expand_as(action_mean)
            return action_mean, action_logstd, self.critic(x)
        else:
            return torch.softmax(self.actor(x), dim=-1), self.critic(x)

# Ejemplo simplificado de A3CWorker
class A3CWorker(mp.Process):
    def __init__(self, global_model, optimizer, env_fn, worker_id, gamma=0.99, tau=0.95, max_steps=1000, update_interval=5, continuous=False, entropy_coef=0.01):
        super().__init__()
        self.worker_id = worker_id
        self.env = env_fn()
        self.gamma = gamma
        self.tau = tau  # Parámetro lambda para GAE
        self.max_steps = max_steps
        self.update_interval = update_interval
        self.continuous = continuous
        self.entropy_coef = entropy_coef

        input_dim = self.env.observation_space.shape[0]
        if continuous:
            action_dim = self.env.action_space.shape[0]
            self.local_model = ActorCritic(input_dim, 256, continuous=True, action_dim=action_dim)
        else:
            n_actions = self.env.action_space.n
            self.local_model = ActorCritic(input_dim, 256, n_actions=n_actions, continuous=False)

        self.global_model = global_model
        self.optimizer = optimizer
        self.buffer = ExperienceBuffer()

    def compute_gae(self, next_value, rewards, masks, values):
        values = values + [next_value]
        gae = 0
        returns = []

        for step in reversed(range(len(rewards))):
            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]
            gae = delta + self.gamma * self.tau * masks[step] * gae
            returns.insert(0, gae + values[step])

        return returns

    def _compute_loss(self, states, actions, returns):
        # Implementar cálculo de pérdida (este es un ejemplo simplificado)
        if self.continuous:
            mean, logstd, values = self.local_model(states)
            std = torch.exp(logstd)
            dist = torch.distributions.Normal(mean, std)
            log_probs = dist.log_prob(actions).sum(dim=-1)
        else:
            probs, values = self.local_model(states)
            dist = torch.distributions.Categorical(probs)
            log_probs = dist.log_prob(actions)

        advantage = returns - values.squeeze()
        actor_loss = -(log_probs * advantage.detach()).mean()
        critic_loss = advantage.pow(2).mean()
        loss = actor_loss + critic_loss  # Se pueden agregar otros términos, como entropía

        return loss

    def run(self):
        total_step = 0
        episode = 0

        while total_step < self.max_steps:
            episode += 1
            episode_reward = 0
            done = False
            state = self.env.reset()

            self.local_model.load_state_dict(self.global_model.state_dict())
            step = 0

            while not done and step < self.max_steps:
                step += 1
                total_step += 1

                for _ in range(self.update_interval):
                    if self.continuous:
                        mean, logstd, value = self.local_model(state)
                        value = value.item()
                        std = torch.exp(logstd)
                        dist = torch.distributions.Normal(mean, std)
                        action = dist.sample()
                        action_log_prob = dist.log_prob(action).sum().item()
                        action = action.detach().numpy()
                    else:
                        probs, value = self.local_model(state)
                        value = value.item()
                        dist = torch.distributions.Categorical(probs)
                        action = dist.sample()
                        action_log_prob = dist.log_prob(action).item()
                        action = action.item()

                    next_state, reward, done, info = self.env.step(action)
                    episode_reward += reward
                    self.buffer.add(state, action, reward, 1.0 - float(done), value, action_log_prob)
                    state = next_state
                    if done:
                        break

                if done:
                    next_value = 0.0
                else:
                    _, next_value = self.local_model(state) if not self.continuous else (None, self.local_model(state)[2])
                    next_value = next_value.item()

                returns = self.compute_gae(next_value, self.buffer.rewards,
                                           [1.0 - d for d in self.buffer.dones],
                                           self.buffer.values)

                states = torch.FloatTensor(self.buffer.states)
                if self.continuous:
                    actions = torch.FloatTensor(self.buffer.actions)
                else:
                    actions = torch.LongTensor(self.buffer.actions)
                returns = torch.FloatTensor(returns)

                loss = self._compute_loss(states, actions, returns)

                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), 0.5)
                for global_param, local_param in zip(self.global_model.parameters(), self.local_model.parameters()):
                    if global_param.grad is None:
                        global_param._grad = local_param.grad
                self.optimizer.step()
                self.buffer.clear()
                self.local_model.load_state_dict(self.global_model.state_dict())

# Crear el entorno y los workers
env = AircraftControlEnvDiscrete(data_csv='/content/processed_aircraft_data.csv', db_path='/content/Data2.db')

global_model = ActorCritic(input_dim=env.observation_space.shape[0], n_actions=env.action_space.n)
optimizer = optim.Adam(global_model.parameters(), lr=3e-4)

workers = []
for worker_id in range(8):  # Usamos 8 workers
    worker = A3CWorker(global_model, optimizer, lambda: env, worker_id)
    workers.append(worker)

for worker in workers:
    worker.start()

for worker in workers:
    worker.join()

import pandas as pd

data = pd.read_csv('/content/processed_aircraft_data.csv')
print(data.head())

def run(self):
    total_step = 0
    episode = 0

    while total_step < self.max_steps:
        episode += 1
        episode_reward = 0
        done = False
        state = self.env.reset()

        self.local_model.load_state_dict(self.global_model.state_dict())
        step = 0

        while not done and step < self.max_steps:
            step += 1
            total_step += 1

            for _ in range(self.update_interval):
                # Selección de acción según el tipo
                if self.continuous:
                    mean, logstd, value = self.local_model(state)
                    value = value.item()
                    std = torch.exp(logstd)
                    dist = torch.distributions.Normal(mean, std)
                    action = dist.sample()
                    action_log_prob = dist.log_prob(action).sum().item()
                    action = action.detach().numpy()
                else:
                    probs, value = self.local_model(state)
                    value = value.item()
                    dist = torch.distributions.Categorical(probs)
                    action = dist.sample()
                    action_log_prob = dist.log_prob(action).item()
                    action = action.item()

                next_state, reward, done, info = self.env.step(action)
                episode_reward += reward
                self.buffer.add(state, action, reward, 1.0 - float(done), value, action_log_prob)
                state = next_state
                if done:
                    break

            if done:
                next_value = 0.0
            else:
                _, next_value = self.local_model(state) if not self.continuous else (None, self.local_model(state)[2])
                next_value = next_value.item()

            returns = self.compute_gae(next_value, self.buffer.rewards,
                                       [1.0 - d for d in self.buffer.dones],
                                       self.buffer.values)

            states = torch.FloatTensor(self.buffer.states)
            if self.continuous:
                actions = torch.FloatTensor(self.buffer.actions)
            else:
                actions = torch.LongTensor(self.buffer.actions)
            returns = torch.FloatTensor(returns)

            loss = self._compute_loss(states, actions, returns)

            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), 0.5)
            for global_param, local_param in zip(self.global_model.parameters(), self.local_model.parameters()):
                if global_param.grad is None:
                    global_param._grad = local_param.grad
            self.optimizer.step()
            self.buffer.clear()
            self.local_model.load_state_dict(self.global_model.state_dict())

        if episode % 10 == 0:
            print(f"[Worker {self.worker_id}] Episodio {episode}: recompensa acumulada = {episode_reward}")

import torch
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp
import numpy as np
from collections import namedtuple

# Estructura para el buffer de experiencia
Trajectory = namedtuple('Trajectory', ['states', 'actions', 'rewards', 'dones', 'values', 'action_log_probs'])

class Buffer:
    def __init__(self):
        self.clear()

    def clear(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.values = []
        self.action_log_probs = []

    def add(self, state, action, reward, done, value, action_log_prob):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.dones.append(done)
        self.values.append(value)
        self.action_log_probs.append(action_log_prob)

class ActorCritic(nn.Module):
    def __init__(self, input_dim, hidden_dim, n_actions, continuous=False):
        super().__init__()
        self.continuous = continuous

        self.shared = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        if self.continuous:
            self.actor_mean = nn.Linear(hidden_dim, n_actions)
            self.actor_logstd = nn.Parameter(torch.zeros(n_actions))
        else:
            self.actor = nn.Linear(hidden_dim, n_actions)

        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.shared(x)
        if self.continuous:
            mean = self.actor_mean(x)
            logstd = self.actor_logstd.expand_as(mean)
            value = self.critic(x)
            return mean, logstd, value
        else:
            probs = torch.softmax(self.actor(x), dim=-1)
            value = self.critic(x)
            return probs, value

class A3CWorker(mp.Process):
    def __init__(self, global_model, env_fn, worker_id, config):
        super().__init__()
        self.global_model = global_model
        self.env = env_fn()
        self.worker_id = worker_id
        self.config = config

        # Configuración
        self.gamma = config.get('gamma', 0.99)
        self.gae_lambda = config.get('gae_lambda', 0.95)
        self.entropy_coef = config.get('entropy_coef', 0.01)
        self.max_steps = config.get('max_steps', 1e6)
        self.update_interval = config.get('update_interval', 5)
        self.continuous = config.get('continuous', False)

        self.local_model = ActorCritic(
            input_dim=self.env.observation_space.shape[0],
            hidden_dim=config['hidden_dim'],
            n_actions=self.env.action_space.shape[0] if continuous else self.env.action_space.n,
            continuous=self.continuous
        )

        self.optimizer = optim.Adam(self.global_model.parameters(), lr=config['lr'])
        self.buffer = Buffer()

    def compute_gae(self, next_value, rewards, dones, values):
        gae = 0
        returns = []
        values = values + [next_value]

        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * values[t+1] * dones[t] - values[t]
            gae = delta + self.gamma * self.gae_lambda * dones[t] * gae
            returns.insert(0, gae + values[t])

        return returns

    def _compute_loss(self, states, actions, returns):
        if self.continuous:
            means, logstds, values = self.local_model(states)
            dist = torch.distributions.Normal(means, torch.exp(logstds))
            log_probs = dist.log_prob(actions).sum(-1)
            entropy = dist.entropy().mean()
        else:
            probs, values = self.local_model(states)
            dist = torch.distributions.Categorical(probs)
            log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()

        advantages = returns - values.squeeze()
        policy_loss = -(log_probs * advantages.detach()).mean()
        value_loss = advantages.pow(2).mean()
        entropy_loss = -entropy * self.entropy_coef

        return policy_loss + 0.5 * value_loss + entropy_loss

    def run(self):
        total_step = 0
        episode = 0

        while total_step < self.max_steps:
            episode += 1
            episode_reward = 0
            done = False
            state = self.env.reset()
            self.local_model.load_state_dict(self.global_model.state_dict())

            while not done and total_step < self.max_steps:
                step = 0
                self.buffer.clear()

                # Recolección de experiencias
                while not done and step < self.update_interval:
                    state = torch.FloatTensor(state)

                    if self.continuous:
                        mean, logstd, value = self.local_model(state)
                        std = torch.exp(logstd)
                        dist = torch.distributions.Normal(mean, std)
                        action = dist.sample()
                        action_log_prob = dist.log_prob(action).sum().item()
                        action = action.detach().numpy()
                        value = value.item()
                    else:
                        probs, value = self.local_model(state)
                        dist = torch.distributions.Categorical(probs)
                        action = dist.sample()
                        action_log_prob = dist.log_prob(action).item()
                        action = action.item()
                        value = value.item()

                    next_state, reward, done, _ = self.env.step(action)
                    episode_reward += reward

                    self.buffer.add(
                        state.numpy(),
                        action,
                        reward,
                        1.0 - float(done),
                        value,
                        action_log_prob
                    )

                    state = next_state
                    step += 1
                    total_step += 1

                # Cálculo de GAE
                with torch.no_grad():
                    if done:
                        next_value = 0.0
                    else:
                        if self.continuous:
                            _, _, next_value = self.local_model(torch.FloatTensor(state))
                        else:
                            _, next_value = self.local_model(torch.FloatTensor(state))
                        next_value = next_value.item()

                returns = self.compute_gae(
                    next_value,
                    self.buffer.rewards,
                    self.buffer.dones,
                    self.buffer.values
                )

                # Convertir a tensores
                states_tensor = torch.FloatTensor(np.array(self.buffer.states))
                returns_tensor = torch.FloatTensor(returns)

                if self.continuous:
                    actions_tensor = torch.FloatTensor(np.array(self.buffer.actions))
                else:
                    actions_tensor = torch.LongTensor(np.array(self.buffer.actions))

                # Calcular pérdida
                loss = self._compute_loss(states_tensor, actions_tensor, returns_tensor)

                # Actualizar modelo global
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), 0.5)

                # Sincronizar gradientes
                for global_param, local_param in zip(self.global_model.parameters(),
                                                   self.local_model.parameters()):
                    if global_param.grad is not None:
                        global_param._grad = local_param.grad.clone()

                self.optimizer.step()
                self.local_model.load_state_dict(self.global_model.state_dict())

            # Logging
            if episode % 10 == 0:
                print(f"[Worker {self.worker_id}] Episodio {episode}: "
                      f"Recompensa: {episode_reward:.2f}, "
                      f"Pasos totales: {total_step}")

# Configuración principal
config = {
    'hidden_dim': 128,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'lr': 2e-4,
    'entropy_coef': 0.01,
    'max_steps': 1e6,
    'update_interval': 5,
    'continuous': False  # Cambiar a True para entornos continuos
}

def train_a3c(env_fn, n_workers=4, config=config):
    global_model = ActorCritic(
        input_dim=env_fn().observation_space.shape[0],
        hidden_dim=config['hidden_dim'],
        n_actions=env_fn().action_space.shape[0] if config['continuous'] else env_fn().action_space.n,
        continuous=config['continuous']
    )
    global_model.share_memory()

    workers = [A3CWorker(global_model, env_fn, i, config) for i in range(n_workers)]
    [w.start() for w in workers]
    [w.join() for w in workers]

    return global_model